{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e942d38f-9e4d-41ef-bda3-377a23cbcdbb",
   "metadata": {},
   "source": [
    "Student: Mariam Valladares-Castellanos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f36434-dbfe-444a-b9f6-8209745e3d0b",
   "metadata": {},
   "source": [
    "# **Automating Historic Nutrient Delivery Model Calibration to Assess Nutrient Retention Services in Puerto Rico**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049c9fd-e545-41b2-a988-dc864bfedf99",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Invest process Sources: \n",
    "\n",
    "https://github.com/ligiambc/campanhao-and-ranieri-2023/blob/main/scriptF.py\n",
    "\n",
    "https://invest.readthedocs.io/en/latest/scripting.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded406b-f747-4b46-a861-5f68ea0d404f",
   "metadata": {},
   "source": [
    "### 1. Preparing ENV and Installing Packages: In anaconda prompt or app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff1658-3e82-4f9d-9e9c-30ecee61dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a new environment\n",
    "!conda create -n  myclone-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7865db-8870-4f9f-8865-dd9bd824869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate the invest-environment\n",
    "!conda activate  myclone-env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8557e4b-eb5a-4f34-aa00-1e1336c42677",
   "metadata": {},
   "source": [
    "*Note*: Make sure jupyther lab is install in the invest-env environment before launch it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4013a4-e693-416f-89d8-43e3064ac4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install conda-channel (github.com/conda-forge/natcap.invest-feestock) in the base environment\n",
    "# or add in app channel with the link https://conda.anaconda.org/conda-forge/\n",
    "!conda config --add channels conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db81934-5586-4378-9306-fa92e1f72d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#install packages (dependencies)\n",
    "# or in anaconda app\n",
    "!pip install gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93de524-8c9a-4a77-9aa8-74efd108a9df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unable to create process using \\'C:\\\\Users\\\\Mariam Valladares\\\\.conda\\\\envs\\\\invest-env\\\\python.exe \"C:\\\\Users\\\\Mariam Valladares\\\\.conda\\\\envs\\\\invest-env\\\\Scripts\\\\pip-script.py\" install natcap.invest\\'']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install natcap invest\n",
    "# or in anaconda app\n",
    "!!pip install natcap.invest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef4f776-9eb3-40c1-bec5-6a46c95c15c4",
   "metadata": {},
   "source": [
    "*Note*: Or install in the anaconda app. After install the package restart the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb839cab-ab51-4a7a-abe4-25c5ba877a8e",
   "metadata": {},
   "source": [
    "*Note*: Install \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79addd57-b5cb-4314-b16b-cf62723eaba7",
   "metadata": {},
   "source": [
    "### 2. Prepare batch Calibration Code for a single parameter and one group of cluster watersheds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81532877-5b39-4477-b3e8-abf319edd1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script used to automate the calibration of the NDR Model Adapted from Campanhao et al. 2023\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import natcap.invest.ndr.ndr\n",
    "import natcap.invest.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7461dd73-4702-4e0c-9456-c575c3120a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up the parameters and folder location\n",
    "#Preparing to test TFA changes with only one cluster of watersheds (cluster 1) for 1951 LULC \n",
    "LOGGER = logging.getLogger(__name__)\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter(\n",
    "    fmt=natcap.invest.utils.LOG_FMT,\n",
    "    datefmt='%m/%d/%Y %H:%M:%S ')\n",
    "handler.setFormatter(formatter)\n",
    "logging.basicConfig(level=logging.INFO, handlers=[handler])\n",
    "\n",
    "args = {\n",
    "    'biophysical_table_path': 'C:/Users/mvalla3/Dropbox/PC/Desktop/Aplicaciones/LSU Research/Chapter 2 Fragmentation and land cover/INVEST/Batch Calibartion NDR/Inputs/NDR_biophys_table_summarize.csv',\n",
    "    'calc_n': True,\n",
    "    'calc_p': True,\n",
    "    'dem_path': 'C:/Users/mvalla3/Dropbox/PC/Desktop/Aplicaciones/LSU Research/Chapter 2 Fragmentation and land cover/INVEST/Batch Calibartion NDR/Inputs/DEM/DEMFill_ASTER.tif',\n",
    "    'k_param': '2',\n",
    "    'lulc_path': 'C:/Users/mvalla3/Dropbox/PC/Desktop/Aplicaciones/LSU Research/Chapter 2 Fragmentation and land cover/INVEST/Batch Calibartion NDR/Inputs/LULC 1951-2000/LULC_1951.tif',\n",
    "    'results_suffix': 'TFA_test',\n",
    "    'runoff_proxy_path': 'C:/Users/mvalla3/Dropbox/PC/Desktop/Aplicaciones/LSU Research/Chapter 2 Fragmentation and land cover/INVEST/Batch Calibartion NDR/Inputs/RunoffProxy/Q_mm_43.tif',\n",
    "    'subsurface_critical_length_n': '200',\n",
    "    'subsurface_eff_n': '0.8',\n",
    "    'threshold_flow_accumulation': '1000',\n",
    "    'watersheds_path': 'C:/Users/mvalla3/Dropbox/PC/Desktop/Aplicaciones/LSU Research/Chapter 2 Fragmentation and land cover/INVEST/Batch Calibartion NDR/Inputs/Drainage areas Group/Ref_WA_Cluster1.shp',\n",
    "    'workspace_dir': 'C:/Users/mvalla3/Dropbox/PC/Desktop/Aplicaciones/LSU Research/Chapter 2 Fragmentation and land cover/INVEST/Batch Calibartion NDR/Output_TFA_test',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "235127ed-078e-4f06-aaa1-1cdab8780ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/03/2024 13:42:33  (pygeoprocessing.geoprocessing) geoprocessing.reproject_vector(2238) INFO starting reprojection\n",
      "12/03/2024 13:42:34  (pygeoprocessing.geoprocessing) geoprocessing.reproject_vector(2286) INFO reprojection 100.0% complete on watershed_results_ndr_TFA_test.gpkg\n",
      "12/03/2024 13:42:34  (pygeoprocessing.geoprocessing) geoprocessing.align_and_resize_raster_stack(1142) INFO 1 of 3 aligned: aligned_dem_TFA_test.tif\n",
      "12/03/2024 13:42:35  (pygeoprocessing.geoprocessing) geoprocessing.align_and_resize_raster_stack(1142) INFO 2 of 3 aligned: aligned_lulc_TFA_test.tif\n",
      "12/03/2024 13:42:35  (pygeoprocessing.geoprocessing) geoprocessing.align_and_resize_raster_stack(1142) INFO 3 of 3 aligned: aligned_runoff_proxy_TFA_test.tif\n",
      "12/03/2024 13:42:35  (pygeoprocessing.geoprocessing) geoprocessing.align_and_resize_raster_stack(1146) INFO aligned all 3 rasters.\n",
      "12/03/2024 13:42:35  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:42:35  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-51 (stats_worker), started daemon 18488)>\n",
      "12/03/2024 13:42:35  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:42:35  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-52 (stats_worker), started daemon 18300)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\numpy\\core\\numeric.py:330: RuntimeWarning: invalid value encountered in cast\n",
      "  multiarray.copyto(a, fill_value, casting='unsafe')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-53 (stats_worker), started daemon 27564)>\n",
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:42:36  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:42:45  (pygeoprocessing.routing.routing) Task._call(1093) INFO (fill pits): complete\n",
      "12/03/2024 13:43:13  (pygeoprocessing.routing.routing) Task._call(1093) INFO 0.0% complete\n",
      "12/03/2024 13:43:16  (pygeoprocessing.routing.routing) Task._call(1093) INFO Flow dir MFD 100.0% complete\n",
      "12/03/2024 13:43:26  (pygeoprocessing.routing.routing) Task._call(1093) INFO Flow accum MFD 100.0% complete\n",
      "12/03/2024 13:43:33  (pygeoprocessing.routing.routing) Task._call(1093) INFO Extract streams MFD: filter out incomplete divergent streams\n",
      "12/03/2024 13:43:33  (pygeoprocessing.routing.routing) Task._call(1093) INFO Extract streams MFD: 100.0% complete\n",
      "12/03/2024 13:43:34  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:34  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-54 (stats_worker), started daemon 17284)>\n",
      "12/03/2024 13:43:34  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:34  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:34  (pygeoprocessing.geoprocessing) geoprocessing.raster_reduce(793) INFO 100.0%% complete\n",
      "12/03/2024 13:43:34  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:34  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-55 (stats_worker), started daemon 4024)>\n",
      "12/03/2024 13:43:35  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:35  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:45  (pygeoprocessing.routing.routing) Task._call(1093) INFO Flow accum MFD 100.0% complete\n",
      "12/03/2024 13:43:45  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:45  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-56 (stats_worker), started daemon 28024)>\n",
      "12/03/2024 13:43:46  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:46  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:47  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:47  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-57 (stats_worker), started daemon 26104)>\n",
      "12/03/2024 13:43:48  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:48  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:48  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:48  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-58 (stats_worker), started daemon 4584)>\n",
      "12/03/2024 13:43:48  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:48  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:53  (pygeoprocessing.routing.routing) Task._call(1093) INFO Dist to channel MFD 100.0% complete\n",
      "12/03/2024 13:43:56  (pygeoprocessing.routing.routing) Task._call(1093) INFO Dist to channel MFD 100.0% complete\n",
      "12/03/2024 13:43:56  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:56  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-59 (stats_worker), started daemon 9180)>\n",
      "12/03/2024 13:43:57  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:57  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:57  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:57  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-60 (stats_worker), started daemon 136)>\n",
      "12/03/2024 13:43:57  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:57  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-61 (stats_worker), started daemon 11616)>\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-62 (stats_worker), started daemon 18684)>\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:58  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:59  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:59  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-63 (stats_worker), started daemon 7400)>\n",
      "12/03/2024 13:43:59  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:43:59  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:43:59  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:43:59  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-64 (stats_worker), started daemon 28896)>\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-65 (stats_worker), started daemon 28476)>\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:00  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-66 (stats_worker), started daemon 11332)>\n",
      "12/03/2024 13:44:01  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:01  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:11  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:11  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-67 (stats_worker), started daemon 25680)>\n",
      "12/03/2024 13:44:11  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:11  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:11  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:11  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-68 (stats_worker), started daemon 27564)>\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-69 (stats_worker), started daemon 29560)>\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:12  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-70 (stats_worker), started daemon 23044)>\n",
      "12/03/2024 13:44:13  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:13  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:13  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:13  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-71 (stats_worker), started daemon 12876)>\n",
      "12/03/2024 13:44:14  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:14  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:14  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(428) INFO starting stats_worker\n",
      "12/03/2024 13:44:14  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(434) INFO started stats_worker <Thread(Thread-72 (stats_worker), started daemon 22472)>\n",
      "12/03/2024 13:44:15  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(513) INFO 100.0% complete\n",
      "12/03/2024 13:44:15  (pygeoprocessing.geoprocessing) geoprocessing.raster_calculator(516) INFO Waiting for raster stats worker result.\n",
      "12/03/2024 13:44:15  (pygeoprocessing.geoprocessing) geoprocessing.reproject_vector(2238) INFO starting reprojection\n",
      "12/03/2024 13:44:15  (pygeoprocessing.geoprocessing) geoprocessing.reproject_vector(2286) INFO reprojection 100.0% complete on reprojected.gpkg\n",
      "12/03/2024 13:44:15  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1775) INFO Clipping rasters to their intersection with the vector\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1800) INFO calculating disjoint polygon sets\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.calculate_disjoint_polygon_set(2816) INFO build shapely polygon list\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.calculate_disjoint_polygon_set(2842) INFO build shapely rtree index\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.calculate_disjoint_polygon_set(2855) INFO poly feature lookup 100.0% complete on watershed_results_ndr_TFA_test.gpkg\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.calculate_disjoint_polygon_set(2859) INFO build poly intersection lookup\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.calculate_disjoint_polygon_set(2885) INFO poly intersection feature lookup 100.0% complete on watershed_results_ndr_TFA_test.gpkg\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.calculate_disjoint_polygon_set(2917) INFO maximal subset build 100.0% complete on watershed_results_ndr_TFA_test.gpkg\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1836) INFO calculating stats on raster 0 of 1\n",
      "12/03/2024 13:44:16  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1846) INFO disjoint polygon set 1 of 4\n",
      "12/03/2024 13:44:17  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1846) INFO disjoint polygon set 2 of 4\n",
      "12/03/2024 13:44:17  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1846) INFO disjoint polygon set 3 of 4\n",
      "12/03/2024 13:44:17  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1846) INFO disjoint polygon set 4 of 4\n",
      "12/03/2024 13:44:17  (pygeoprocessing.geoprocessing) geoprocessing.zonal_statistics(1975) INFO all done processing polygon sets for watershed_results_ndr_TFA_test.gpkg\n",
      "12/03/2024 13:44:17  (taskgraph.Task) Task.add_task(706) ERROR Something went wrong when adding task aggregate n subsurface export (33), terminating taskgraph.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\taskgraph\\Task.py\", line 674, in add_task\n",
      "    new_task._call()\n",
      "  File \"C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\taskgraph\\Task.py\", line 1093, in _call\n",
      "    payload = self._func(*self._args, **self._kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\natcap\\invest\\ndr\\ndr.py\", line 1438, in _aggregate_and_pickle_total\n",
      "    result = pygeoprocessing.zonal_statistics(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/pygeoprocessing/geoprocessing_core.pyx\", line 77, in pygeoprocessing.geoprocessing_core.gdal_use_exceptions.wrapper\n",
      "  File \"src/pygeoprocessing/geoprocessing_core.pyx\", line 78, in pygeoprocessing.geoprocessing_core.gdal_use_exceptions.wrapper\n",
      "  File \"C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\pygeoprocessing\\geoprocessing.py\", line 1982, in zonal_statistics\n",
      "    shutil.rmtree(temp_working_dir)\n",
      "  File \"C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\shutil.py\", line 820, in rmtree\n",
      "    return _rmtree_unsafe(path, onexc)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\shutil.py\", line 652, in _rmtree_unsafe\n",
      "    onexc(os.rmdir, path, err)\n",
      "  File \"C:\\Users\\mvalla3\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\shutil.py\", line 650, in _rmtree_unsafe\n",
      "    os.rmdir(path)\n",
      "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\mvalla3\\\\Dropbox\\\\PC\\\\Desktop\\\\Aplicaciones\\\\LSU Research\\\\Chapter 2 Fragmentation and land cover\\\\INVEST\\\\Batch Calibartion NDR\\\\Output_TFA_test\\\\intermediate_outputs\\\\tmpmd0giqe2'\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\mvalla3\\\\Dropbox\\\\PC\\\\Desktop\\\\Aplicaciones\\\\LSU Research\\\\Chapter 2 Fragmentation and land cover\\\\INVEST\\\\Batch Calibartion NDR\\\\Output_TFA_test\\\\intermediate_outputs\\\\tmpmd0giqe2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#set the suffix to be accum### for the current threshold_flow_accumulation\u001b[39;00m\n\u001b[0;32m      8\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(threshold_flow_accumulation)\n\u001b[1;32m----> 9\u001b[0m natcap\u001b[38;5;241m.\u001b[39minvest\u001b[38;5;241m.\u001b[39mndr\u001b[38;5;241m.\u001b[39mndr\u001b[38;5;241m.\u001b[39mexecute(args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\natcap\\invest\\ndr\\ndr.py:954\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# only need to calculate total for nitrogen because\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;66;03m# phosphorus only has surface export\u001b[39;00m\n\u001b[0;32m    942\u001b[0m total_export_task \u001b[38;5;241m=\u001b[39m task_graph\u001b[38;5;241m.\u001b[39madd_task(\n\u001b[0;32m    943\u001b[0m     func\u001b[38;5;241m=\u001b[39mpygeoprocessing\u001b[38;5;241m.\u001b[39mraster_map,\n\u001b[0;32m    944\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    951\u001b[0m         surface_export_task, subsurface_export_task],\n\u001b[0;32m    952\u001b[0m     task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal export n\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 954\u001b[0m _ \u001b[38;5;241m=\u001b[39m task_graph\u001b[38;5;241m.\u001b[39madd_task(\n\u001b[0;32m    955\u001b[0m     func\u001b[38;5;241m=\u001b[39m_aggregate_and_pickle_total,\n\u001b[0;32m    956\u001b[0m     args\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    957\u001b[0m         (f_reg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_subsurface_export_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    958\u001b[0m         f_reg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwatershed_results_ndr_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    959\u001b[0m         f_reg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsurface_export_n_pickle_path\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m    960\u001b[0m     target_path_list\u001b[38;5;241m=\u001b[39m[f_reg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsurface_export_n_pickle_path\u001b[39m\u001b[38;5;124m'\u001b[39m]],\n\u001b[0;32m    961\u001b[0m     dependent_task_list\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    962\u001b[0m         subsurface_export_task, create_vector_task],\n\u001b[0;32m    963\u001b[0m     task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregate n subsurface export\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    965\u001b[0m _ \u001b[38;5;241m=\u001b[39m task_graph\u001b[38;5;241m.\u001b[39madd_task(\n\u001b[0;32m    966\u001b[0m     func\u001b[38;5;241m=\u001b[39m_aggregate_and_pickle_total,\n\u001b[0;32m    967\u001b[0m     args\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    973\u001b[0m     dependent_task_list\u001b[38;5;241m=\u001b[39m[total_export_task, create_vector_task],\n\u001b[0;32m    974\u001b[0m     task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregate n total export\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    976\u001b[0m _ \u001b[38;5;241m=\u001b[39m task_graph\u001b[38;5;241m.\u001b[39madd_task(\n\u001b[0;32m    977\u001b[0m     func\u001b[38;5;241m=\u001b[39m_aggregate_and_pickle_total,\n\u001b[0;32m    978\u001b[0m     args\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m     dependent_task_list\u001b[38;5;241m=\u001b[39m[subsurface_load_task, create_vector_task],\n\u001b[0;32m    985\u001b[0m     task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnutrient\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m subsurface load\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\taskgraph\\Task.py:674\u001b[0m, in \u001b[0;36mTaskGraph.add_task\u001b[1;34m(self, func, args, kwargs, task_name, target_path_list, ignore_path_list, hash_target_files, dependent_task_list, ignore_directories, priority, hash_algorithm, transient_run, store_result)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_hash_map[new_task] \u001b[38;5;241m=\u001b[39m new_task\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_workers \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;66;03m# call directly if single threaded\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     new_task\u001b[38;5;241m.\u001b[39m_call()\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;66;03m# determine if task is ready or is dependent on other\u001b[39;00m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;66;03m# tasks\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    679\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultithreaded: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m sending to new task queue.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    680\u001b[0m         task_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\taskgraph\\Task.py:1093\u001b[0m, in \u001b[0;36mTask._call\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1092\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect _func for task \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name)\n\u001b[1;32m-> 1093\u001b[0m     payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_result:\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m payload\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\natcap\\invest\\ndr\\ndr.py:1438\u001b[0m, in \u001b[0;36m_aggregate_and_pickle_total\u001b[1;34m(base_raster_path_band, aggregate_vector_path, target_pickle_path)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aggregate_and_pickle_total\u001b[39m(\n\u001b[0;32m   1423\u001b[0m         base_raster_path_band, aggregate_vector_path, target_pickle_path):\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Aggregate base raster path to vector path FIDs and pickle result.\u001b[39;00m\n\u001b[0;32m   1425\u001b[0m \n\u001b[0;32m   1426\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \n\u001b[0;32m   1437\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1438\u001b[0m     result \u001b[38;5;241m=\u001b[39m pygeoprocessing\u001b[38;5;241m.\u001b[39mzonal_statistics(\n\u001b[0;32m   1439\u001b[0m         base_raster_path_band, aggregate_vector_path,\n\u001b[0;32m   1440\u001b[0m         working_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(target_pickle_path))\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(target_pickle_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target_pickle_file:\n\u001b[0;32m   1443\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(result, target_pickle_file)\n",
      "File \u001b[1;32msrc/pygeoprocessing/geoprocessing_core.pyx:77\u001b[0m, in \u001b[0;36mpygeoprocessing.geoprocessing_core.gdal_use_exceptions.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc/pygeoprocessing/geoprocessing_core.pyx:78\u001b[0m, in \u001b[0;36mpygeoprocessing.geoprocessing_core.gdal_use_exceptions.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\site-packages\\pygeoprocessing\\geoprocessing.py:1982\u001b[0m, in \u001b[0;36mzonal_statistics\u001b[1;34m(base_raster_path_band, aggregate_vector_path, aggregate_layer_name, ignore_nodata, polygons_might_overlap, include_value_counts, working_dir)\u001b[0m\n\u001b[0;32m   1979\u001b[0m data_band, data_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m disjoint_layer, target_layer, target_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1982\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_working_dir)\n\u001b[0;32m   1983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_raster_mode:\n\u001b[0;32m   1984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aggregate_stats_list\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\shutil.py:820\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;66;03m# can't continue even if onexc hook returns\u001b[39;00m\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _rmtree_unsafe(path, onexc)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\shutil.py:652\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    650\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 652\u001b[0m     onexc(os\u001b[38;5;241m.\u001b[39mrmdir, path, err)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\myclone-env\\Lib\\shutil.py:650\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onexc)\u001b[0m\n\u001b[0;32m    648\u001b[0m             onexc(os\u001b[38;5;241m.\u001b[39munlink, fullname, err)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     os\u001b[38;5;241m.\u001b[39mrmdir(path)\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    652\u001b[0m     onexc(os\u001b[38;5;241m.\u001b[39mrmdir, path, err)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\mvalla3\\\\Dropbox\\\\PC\\\\Desktop\\\\Aplicaciones\\\\LSU Research\\\\Chapter 2 Fragmentation and land cover\\\\INVEST\\\\Batch Calibartion NDR\\\\Output_TFA_test\\\\intermediate_outputs\\\\tmpmd0giqe2'"
     ]
    }
   ],
   "source": [
    "# Test to evaluate one parameter iteration for the calibration process for TFA Between 50 and 10000\n",
    "if __name__ == '__main__':\n",
    "    #Loops through the values 50 to 3000\n",
    "    for threshold_flow_accumulation in range(50, 3000, 50):\n",
    "        #set the accumulation threshold to the current value in the loop\n",
    "        args['threshold_flow_accumulation'] = threshold_flow_accumulation\n",
    "        #set the suffix to be accum### for the current threshold_flow_accumulation\n",
    "        args['suffix'] = 'accum' + str(threshold_flow_accumulation)\n",
    "        natcap.invest.ndr.ndr.execute(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5530b0e-d09c-483d-bed2-5316856a25d6",
   "metadata": {},
   "source": [
    "### 3. Prepare batch Calibration Code for a single parameter and one group of cluster watersheds (in Progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076f094-5e23-4ddc-baa9-76bd38ea9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the values for the Borcelli K and TFA calibration parameters and the years used for calibration\n",
    "\n",
    "years = [1951, 1977, 1991, 2000]\n",
    "kparam = [2, 1.5, 1.4, 1.3, 1.2, 1.1, 1, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "ICparam = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]\n",
    "\n",
    "###Use a “for” loop to change the arguments (k, IC, and years)\n",
    "\n",
    "#Varying k and IC0 together\n",
    "\n",
    "for k in kparam :\n",
    "    args['k_param'] = k\n",
    "    for ic in ICparam :\n",
    "        args['ic_0_param'] = ic\n",
    "        for year in years :\n",
    "            lulc = 'C:/folder/lulc_' + str(year) + '.tif'\n",
    "            erosivity = 'C:/folder/erosivity_' + str(year) + '.tif'\n",
    "            args['lulc_path'] = lulc\n",
    "            args['erosivity_path'] = erosivity\n",
    "            args['results_suffix'] = 'year' + str(year) + '_' + str(k) + '_' + str(ic)\n",
    "            if __name__ == '__main__':\n",
    "                natcap.invest.sdr.sdr.execute(args)\n",
    "\n",
    "#Varying only IC0 (or k)\n",
    "#For varying only k, change ‘ic_0_param’ to ‘k_param’, ICparam to kparam, and ic to k \n",
    "\n",
    "for ic in ICparam :\n",
    "    args['ic_0_param'] = ic\n",
    "    for year in years :\n",
    "        lulc = 'C:/folder/lulc_' + str(year) + '.tif'\n",
    "        erosivity = 'C:/folder/erosivity_' + str(year) + '.tif'\n",
    "        args['lulc_path'] = lulc\n",
    "        args['erosivity_path'] = erosivity\n",
    "        args['results_suffix'] = 'year' + str(year) + '_' + str(ic)\n",
    "        if __name__ == '__main__':\n",
    "            natcap.invest.sdr.sdr.execute(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96af948d-8532-482d-9829-4d88e5308df9",
   "metadata": {},
   "source": [
    "### 4. Extract the desire files to dredge for Invest outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682df368-6247-4c05-a108-93625618ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data frame to store the data\n",
    "\n",
    "list_sed_export <- data.frame()\n",
    "\n",
    "#Set the values for the arguments ‘percentage’ and ‘roughness’ and the number of landscape replicates (shapefiles must have this information in their filenames)\n",
    "\n",
    "percent <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)\n",
    "roughness <- c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9)\n",
    "numberldcp <- 1:100\n",
    "\n",
    "#Use a “for” loop to extract the dbf from the shapefile and add the data to the data frame\n",
    "\n",
    "for (p in percent){\n",
    "  for (r in roughness){\n",
    "    for (n in numberldcp){\n",
    "      arch <- paste('C:/folder/watershed_results_sdr_landscape_p',p, '_r', r, '_', n, '.dbf', sep=\"\")\n",
    "      dbf_shape <- read.dbf(arch)\n",
    "      col_ldcp <- paste('C:/folder/watershed_results_sdr_landscape_p', p, '_r', r, '_', n , '.tif', sep=\"\")\n",
    "      percentage <- paste(p)\n",
    "      roughness_val <- paste(r)\n",
    "      line_ldcp <- cbind(dbf_shape, col_ldcp, percentage, roughness_val)\n",
    "      list_sed_export <- rbind(line_ldcp, list_sed_export)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#Save the table to the hard drive in .csv format\n",
    "\n",
    "write.csv(list_sed_export, file=\"sed_export_landscapes.csv\", quote=FALSE, row.names=FALSE, col.names=TRUE, na=\"NA\", dec=\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
